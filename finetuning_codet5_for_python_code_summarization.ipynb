{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "installing the required libraries and we load the \"code_to_text\" portion of the CodeXGLUE dataset. We only load the examples of the python programming language."
      ],
      "metadata": {
        "id": "lY4dtzynOPUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRw5cTOgjNFy",
        "outputId": "dbda0bcf-22ee-4a7e-da5e-bcf4bf81d40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets\n",
        "!pip install -q pytorch-lightning wandb\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"code-to-text/ruby\" split consists of a training, validation and test set."
      ],
      "metadata": {
        "id": "WE5jR3krONpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset['train'][0]\n",
        "\n",
        "print(\"Code:\", example[\"code\"])\n",
        "print(\"Docstring:\", example[\"docstring\"])"
      ],
      "metadata": {
        "id": "7uWxA_oroKOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert the code and docstring text into input_ids, which are integer representations of tokens from the model's vocabulary.\n",
        "\n",
        "For this:\n",
        "\n",
        "Input: The code is converted into input_ids and attention_mask (to ignore padding).\n",
        "Output: The docstring is also converted into input_ids, which serve as labels.\n",
        "Both inputs and labels are padded/truncated to a uniform length for batch processing. The function preprocess_examples will handle this conversion for the entire dataset."
      ],
      "metadata": {
        "id": "oDw7AOiVWAgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
        "\n",
        "prefix = \"Summarize Ruby: \"\n",
        "max_input_length = 256\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_examples(examples):\n",
        "  # encode the code-docstring pairs\n",
        "  codes = examples['code']\n",
        "  docstrings = examples['docstring']\n",
        "\n",
        "  inputs = [prefix + code for code in codes]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "  # encode the summaries\n",
        "  labels = tokenizer(docstrings, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "  # important: we need to replace the index of the padding tokens by -100\n",
        "  # such that they are not taken into account by the CrossEntropyLoss\n",
        "  labels_with_ignore_index = []\n",
        "  for labels_example in labels:\n",
        "    labels_example = [label if label != 0 else -100 for label in labels_example]\n",
        "    labels_with_ignore_index.append(labels_example)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels_with_ignore_index\n",
        "\n",
        "  return model_inputs\n"
      ],
      "metadata": {
        "id": "ivkCASKroM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Then we call .map() on the HuggingFace Dataset object, which allows us to apply this function in batches"
      ],
      "metadata": {
        "id": "czEFLEu1WDP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess_examples, batched=True)"
      ],
      "metadata": {
        "id": "3l37DvxroQpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "3uvTVKokoX5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set the format to \"torch\" and create PyTorch dataloaders."
      ],
      "metadata": {
        "id": "AfaO_JTLWTcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "train_dataloader = DataLoader(dataset['train'], shuffle=True, batch_size=8)\n",
        "valid_dataloader = DataLoader(dataset['validation'], batch_size=4)\n",
        "test_dataloader = DataLoader(dataset['test'], batch_size=4)"
      ],
      "metadata": {
        "id": "1JK3KbEwoaYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "0RkDdrxKoclE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(batch['input_ids'][0])"
      ],
      "metadata": {
        "id": "7nZCQQweoe7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = batch['labels'][0]\n",
        "tokenizer.decode([label for label in labels if label != -100])"
      ],
      "metadata": {
        "id": "5GRDpQywomC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model with PyTorch Lightning, we'll create a LightningModule, which is like an enhanced nn.Module.\n",
        "\n",
        "In this module, we'll define:\n",
        "\n",
        "forward: The model's forward pass.\n",
        "training_step: What happens during each training step (like calculating loss).\n",
        "validation_step and test_step (optional): For validation and testing.\n",
        "We'll also set up data loaders for training, validation, and testing.\n",
        "\n",
        "PyTorch Lightning will handle the rest, like moving data to the right device, logging, and training automation"
      ],
      "metadata": {
        "id": "whkiEQZIWsb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class CodeT5(pl.LightningModule):\n",
        "    def __init__(self, lr=5e-5, num_train_epochs=15, warmup_steps=1000):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-small\")\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return outputs\n",
        "\n",
        "    def common_step(self, batch, batch_idx):\n",
        "        outputs = self(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.common_step(batch, batch_idx)\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.common_step(batch, batch_idx)\n",
        "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self.common_step(batch, batch_idx)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # create optimizer\n",
        "        optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        # create learning rate scheduler\n",
        "        num_train_optimization_steps = self.hparams.num_train_epochs * len(train_dataloader)\n",
        "        lr_scheduler = {'scheduler': get_linear_schedule_with_warmup(optimizer,\n",
        "                                                    num_warmup_steps=self.hparams.warmup_steps,\n",
        "                                                    num_training_steps=num_train_optimization_steps),\n",
        "                        'name': 'learning_rate',\n",
        "                        'interval':'step',\n",
        "                        'frequency': 1}\n",
        "\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return valid_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return test_dataloader"
      ],
      "metadata": {
        "id": "jp_dCwPDqW1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "intialilizing weights and biases"
      ],
      "metadata": {
        "id": "GzNHqrz7WvB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "gcDw0GR0qdn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initializing the model."
      ],
      "metadata": {
        "id": "oA9H1LVaW6_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CodeT5()"
      ],
      "metadata": {
        "id": "BGlpVaJgqmoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "\n",
        "wandb_logger = WandbLogger(name='codet5-finetune-code-summarization-ruby-shuffle', project='CodeT5')\n",
        "# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='validation_loss',\n",
        "    patience=3,\n",
        "    strict=False,\n",
        "    verbose=False,\n",
        "    mode='min'\n",
        ")\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "trainer = Trainer(\n",
        "                  default_root_dir=\"/content/drive/MyDrive/CodeT5/Notebooks/Checkpoints\",\n",
        "                  logger=wandb_logger,\n",
        "                  callbacks=[early_stop_callback, lr_monitor])\n",
        "trainer.fit(model)"
      ],
      "metadata": {
        "id": "pE6AhnMuqpMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \".\"  # save in the current working directory, you can change this of course\n",
        "model.model.save_pretrained(save_directory)\n"
      ],
      "metadata": {
        "id": "7cgqdASOvN8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained a model, let's test it on some examples from the test set."
      ],
      "metadata": {
        "id": "Of6pGcthXG_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\")\n",
        "print(dataset['test'])"
      ],
      "metadata": {
        "id": "e6sck2RsvPlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_example = dataset['test'][691]\n",
        "print(\"Code:\", test_example['code'])"
      ],
      "metadata": {
        "id": "okpYKZB4vVfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(save_directory)"
      ],
      "metadata": {
        "id": "dcPuDOysvZZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for the model\n",
        "input_ids = tokenizer(test_example['code'], return_tensors='pt').input_ids\n",
        "# generate\n",
        "outputs = model.generate(input_ids)\n",
        "print(\"Generated docstring:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "_vOPlFEhvgDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ground truth:\", test_example['docstring'])"
      ],
      "metadata": {
        "id": "NWjcY6j6vkss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}